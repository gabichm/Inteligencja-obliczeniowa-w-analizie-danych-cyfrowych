{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza uczenia ze wzmocnieniem – środowisko MountainCarContinuous-v0 z algorytmem PPO\n",
    "\n",
    "W niniejszym notebooku przeprowadzono eksperymenty uczenia ze wzmocnieniem w środowisku `MountainCarContinuous-v0` z wykorzystaniem algorytmu PPO (Proximal Policy Optimization). Celem było porównanie wpływu różnych zestawów hiperparametrów i architektur sieci na efektywność treningu.\n"
   ],
   "id": "101fd77b30a57e3b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ],
   "id": "3f5b6b3ed86d9a5d",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charakterystyka środowiska\n",
    "\n",
    "Środowisko `MountainCarContinuous-v0` polega na sterowaniu samochodem poruszającym się po dolinie, tak aby zdobył wystarczającą prędkość i wspiął się na szczyt wzgórza. Akcja to wartość ciągła z zakresu [-1, 1] reprezentująca siłę napędową. Otrzymuje się nagrodę +100 za osiągnięcie celu, jednak środowisko penalizuje za długie próby, przyznając ujemne nagrody proporcjonalne do czasu.\n",
    "\n",
    "Zadanie jest trudne, ponieważ wymaga planowania długoterminowego: samochód musi najpierw zjechać w dół, by nabrać impetu.\n",
    "\n",
    "## Charakterystyka algorytmu PPO\n",
    "\n",
    "Algorytm PPO (Proximal Policy Optimization) należy do grupy metod typu policy gradient i służy do optymalizacji polityki sterowania. Jego kluczową cechą jest zastosowanie ograniczonych, tzw. \"bezpiecznych\" aktualizacji parametrów poprzez mechanizm clipowania funkcji celu, co znacząco poprawia stabilność uczenia się w porównaniu do starszych metod, takich jak REINFORCE czy A2C. PPO sprawdza się dobrze zarówno w środowiskach z przestrzenią akcji ciągłych, jak i dyskretnych, co czyni go popularnym i efektywnym wyborem.\n"
   ],
   "id": "d6ba7efc3e7b77b1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie różnych zestawów hiperparametrów\n",
    "Poniżej znajdują się 3 różne zestawy hiperparemtrów, które będziemy testować, aby ustalić które są najlepsze"
   ],
   "id": "1944b4f9a2716eec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- Ustawienia ---\n",
    "env_id = \"MountainCarContinuous-v0\"\n",
    "total_timesteps = 50000\n",
    "num_runs = 10\n",
    "eval_interval = 5000\n",
    "n_eval_episodes = 10\n",
    "\n",
    "# Zestawy hiperparametrów\n",
    "hyperparams = {\n",
    "    \"A\": {\"learning_rate\": 1e-5, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95, \"n_steps\": 512},\n",
    "    \"B\": {\"learning_rate\": 1e-4, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95, \"n_steps\": 1024},\n",
    "    \"C\": {\"learning_rate\": 1e-3, \"clip_range\": 0.3, \"gamma\": 0.99, \"gae_lambda\": 0.95, \"n_steps\": 2048},\n",
    "}\n"
   ],
   "id": "e9afa1f621ca51f3",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja `train_and_evaluate`\n",
    "\n",
    "* `DummyVecEnv` umożliwia obsługę środowisk wektorowych, które są wymagane przez bibliotekę Stable-Baselines3 do równoległego wykonywania i zarządzania środowiskami.\n",
    "* `VecNormalize` automatycznie normalizuje zarówno obserwacje (`norm_obs=True`), jak i nagrody (`norm_reward=True`).\n",
    "\n",
    "  * Normalizacja obserwacji sprawia, że dane wejściowe do sieci neuronowej mają stabilne rozkłady statystyczne (średnia bliska 0, odchylenie standardowe bliskie 1), co ułatwia proces uczenia.\n",
    "  * Normalizacja nagród zapobiega problemom związanym z bardzo dużymi lub bardzo małymi wartościami nagród, poprawiając stabilność i szybkość uczenia się.\n",
    "* W funkcji tworzymy instancję modelu PPO, który uczy się przez określoną liczbę kroków, regularnie oceniając swoją politykę i zbierając średnie nagrody.\n",
    "* Na koniec symulujemy jeden epizod z wyuczoną polityką w trybie wizualizacji, aby zobaczyć efekt działania modelu.\n"
   ],
   "id": "9f48fc30694e58e9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def make_env(seed):\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def train_and_evaluate(params, seed):\n",
    "    env = DummyVecEnv([make_env(seed)])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0,\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                clip_range=params[\"clip_range\"],\n",
    "                gamma=params[\"gamma\"],\n",
    "                gae_lambda=params[\"gae_lambda\"],\n",
    "                n_steps=params[\"n_steps\"],\n",
    "                seed=seed)\n",
    "\n",
    "    rewards = []\n",
    "    timesteps_so_far = 0\n",
    "    while timesteps_so_far < total_timesteps:\n",
    "        start_time = time.time()\n",
    "        model.learn(total_timesteps=eval_interval, reset_num_timesteps=False)\n",
    "        elapsed = time.time() - start_time\n",
    "        # print(f\"Learned {eval_interval} steps in {elapsed:.2f}s\")\n",
    "        timesteps_so_far += eval_interval\n",
    "\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes, deterministic=True)\n",
    "        rewards.append(mean_reward)\n",
    "\n",
    "    # Symulacja 1 epizodu do wizualizacji\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "    env.close()\n",
    "    return rewards\n"
   ],
   "id": "76f1c832a9d2ea",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def main():\n",
    "    results = {}\n",
    "    steps = np.arange(eval_interval, total_timesteps + 1, eval_interval)\n",
    "\n",
    "    for name, params in hyperparams.items():\n",
    "        print(f\"Trening zestawu {name}...\")\n",
    "        all_runs_rewards = []\n",
    "        for run in range(num_runs):\n",
    "            seed = 1000 + run\n",
    "            rewards = train_and_evaluate(params, seed)\n",
    "            all_runs_rewards.append(rewards)\n",
    "            # print(f\"Run {run+1}/{num_runs} done\")\n",
    "\n",
    "        all_runs_rewards = np.array(all_runs_rewards)\n",
    "        mean_rewards = np.mean(all_runs_rewards, axis=0)\n",
    "        std_rewards = np.std(all_runs_rewards, axis=0)\n",
    "\n",
    "        results[name] = {\"mean\": mean_rewards, \"std\": std_rewards}\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for name in hyperparams:\n",
    "        plt.plot(steps, results[name][\"mean\"], label=f\"Zestaw {name}\")\n",
    "        plt.fill_between(steps,\n",
    "                         results[name][\"mean\"] - results[name][\"std\"],\n",
    "                         results[name][\"mean\"] + results[name][\"std\"],\n",
    "                         alpha=0.2)\n",
    "    plt.title(\"Średnia nagroda vs. liczba kroków czasowych\")\n",
    "    plt.xlabel(\"Kroki czasowe\")\n",
    "    plt.ylabel(\"Średnia nagroda (z 10 epizodów)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "main()\n"
   ],
   "id": "5b93d88d52f7bb4c",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza wyników testowania różnych hiperparametrów \n",
    "\n",
    "W eksperymencie rozważono trzy zestawy hiperparametrów, różniące się głównie współczynnikiem uczenia (`learning_rate`), `n_steps` oraz zakresem `clip_range`.\n",
    "\n",
    "Wyniki wskazują, że zestaw **B (learning_rate = 1e-4)** najszybciej uzyskiwał najlepsze wyniki średniej nagrody, osiągając wartości zbliżone do +90 w ostatnich krokach. Zestaw **A (1e-5)** uczył się zbyt wolno, ale osiągał końcowo bardzo dobre wyniki, natomiast **C (1e-3)** miał tendencję do niestabilnego zachowania, prawdopodobnie przez zbyt agresywną aktualizację.\n",
    "\n",
    "Średni czas potrzebny na przetworzenie 5000 kroków to około 5-6 sekund, co daje ~0.0001 s na krok środowiska.\n",
    "\n",
    "Wnioskiem jest, że dobór umiarkowanego learning rate i rozsądnego `n_steps` (512–1024) jest kluczowy dla skuteczności PPO w tym środowisku.\n"
   ],
   "id": "97f9a56aaaa2eda6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie różnych architektur sieci używanych przez agenta\n",
    "\n",
    "Zostały przetestowane dwie architektury sieci wykorzystywanych przez agenta PPO. W obu przypadkach na wejście sieci trafia dwuelementowy wektor `[pozycja, prędkość]`, a na wyjściu znajduje się wartość ciągła reprezentująca siłę napędu.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Architekture ReLu\n",
    "Ta architektura to klasyczna konfiguracja `[64, 64]` z funkcją aktywacji ReLU. Zapewnia wystarczającą głębokość przy umiarkowanej liczbie parametrów.\n",
    "\n",
    "Wejście: [pozycja, prędkość]  →  (2 neurony)\n",
    "\n",
    "    ┌─────────────┐\n",
    "    │  FC (64)    │  ← ReLU\n",
    "    └─────────────┘\n",
    "          ↓\n",
    "    ┌─────────────┐\n",
    "    │  FC (64)    │  ← ReLU\n",
    "    └─────────────┘\n",
    "          ↓\n",
    "     Wyjście: [akcja ciągła]\n",
    "\n",
    "## Architektura Tanh  (głębsza)\n",
    "Ta architektura to głębsza sieć `[128, 64, 32]` z funkcją aktywacji Tanh, która lepiej odwzorowuje niestacjonarne zależności przy mniejszych gradientach.\n",
    "\n",
    "Wejście: [pozycja, prędkość]  →  (2 neurony)\n",
    "\n",
    "    ┌─────────────┐\n",
    "    │  FC (128)   │  ← Tanh\n",
    "    └─────────────┘\n",
    "          ↓\n",
    "    ┌─────────────┐\n",
    "    │  FC (64)    │  ← Tanh\n",
    "    └─────────────┘\n",
    "          ↓\n",
    "    ┌─────────────┐\n",
    "    │  FC (32)    │  ← Tanh\n",
    "    └─────────────┘\n",
    "          ↓\n",
    "     Wyjście: [akcja ciągła]\n"
   ],
   "id": "b9ac2445b6596b48"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "from torch.nn import Tanh, ReLU\n",
    "\n",
    "hyperparams = {\n",
    "    \"ReLu\": {\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"n_steps\": 1024,\n",
    "        \"policy_kwargs\": dict(activation_fn=ReLU, net_arch=[64, 64])\n",
    "    },\n",
    "    \"Tanh\": {\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"n_steps\": 1024,\n",
    "        \"policy_kwargs\": dict(activation_fn=Tanh, net_arch=[128, 64, 32])\n",
    "    }\n",
    "}\n",
    "def train_and_evaluate_with_kwargs(params, seed):\n",
    "    env = DummyVecEnv([make_env(seed)])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0,\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                clip_range=params[\"clip_range\"],\n",
    "                gamma=params[\"gamma\"],\n",
    "                gae_lambda=params[\"gae_lambda\"],\n",
    "                n_steps=params[\"n_steps\"],\n",
    "                policy_kwargs=params[\"policy_kwargs\"],\n",
    "                seed=seed)\n",
    "\n",
    "    rewards = []\n",
    "    timesteps_so_far = 0\n",
    "    while timesteps_so_far < total_timesteps:\n",
    "        start_time = time.time()\n",
    "        model.learn(total_timesteps=eval_interval, reset_num_timesteps=False)\n",
    "        elapsed = time.time() - start_time\n",
    "        # print(f\"Learned {eval_interval} steps in {elapsed:.2f}s\")\n",
    "        timesteps_so_far += eval_interval\n",
    "\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes, deterministic=True)\n",
    "        rewards.append(mean_reward)\n",
    "\n",
    "    # Symulacja 1 epizodu do wizualizacji\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "    env.close()\n",
    "    return rewards\n"
   ],
   "id": "9e70f7f9e89249cf",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "def main():\n",
    "    results = {}\n",
    "    steps = np.arange(eval_interval, total_timesteps + 1, eval_interval)\n",
    "\n",
    "    for name, params in hyperparams.items():\n",
    "        print(f\"Trening zestawu {name}...\")\n",
    "        all_runs_rewards = []\n",
    "        for run in range(num_runs):\n",
    "            seed = 1000 + run\n",
    "            rewards = train_and_evaluate(params, seed)\n",
    "            all_runs_rewards.append(rewards)\n",
    "            # print(f\"Run {run+1}/{num_runs} done\")\n",
    "\n",
    "        all_runs_rewards = np.array(all_runs_rewards)\n",
    "        mean_rewards = np.mean(all_runs_rewards, axis=0)\n",
    "        std_rewards = np.std(all_runs_rewards, axis=0)\n",
    "\n",
    "        results[name] = {\"mean\": mean_rewards, \"std\": std_rewards}\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for name in hyperparams:\n",
    "        plt.plot(steps, results[name][\"mean\"], label=f\"Zestaw {name}\")\n",
    "        plt.fill_between(steps,\n",
    "                         results[name][\"mean\"] - results[name][\"std\"],\n",
    "                         results[name][\"mean\"] + results[name][\"std\"],\n",
    "                         alpha=0.2)\n",
    "    plt.title(\"Średnia nagroda vs. liczba kroków czasowych\")\n",
    "    plt.xlabel(\"Kroki czasowe\")\n",
    "    plt.ylabel(\"Średnia nagroda (z 10 epizodów)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "main()\n"
   ],
   "id": "76350f81ec5e95b6",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki testów róznych architektur sieci\n",
    "Wyniki eksperymentów pokazują, że architektura Tanh uczy się wolniej, ale osiąga bardziej stabilne wyniki w dłuższym czasie, prawdopodobnie dzięki większej ekspresji reprezentacji. Architektura Relu może szybciej osiągać lokalne optimum, ale ma tendencję do niestabilności."
   ],
   "id": "51ef074d7f129f65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie modelu z wyłączoną eksploracją\n",
    "\n",
    "Do tego testu wzięliśmy hiperparametry agenta B z pierwszych testów, jako że to on osiągał najlepsze wyniki. Na takim agencie puściliśmy symulację z wyłączoną eksploracją\n",
    "\n"
   ],
   "id": "c9d7a308783afd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_and_evaluate_no_exploration(params, seed):\n",
    "    env = DummyVecEnv([make_env(seed)])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "    \n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0,\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                clip_range=params[\"clip_range\"],\n",
    "                gamma=params[\"gamma\"],\n",
    "                gae_lambda=params[\"gae_lambda\"],\n",
    "                n_steps=params[\"n_steps\"],\n",
    "                seed=seed)\n",
    "    \n",
    "    rewards = []\n",
    "    timesteps_so_far = 0\n",
    "    while timesteps_so_far < total_timesteps:\n",
    "        model.learn(total_timesteps=eval_interval, reset_num_timesteps=False)\n",
    "        timesteps_so_far += eval_interval\n",
    "\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes, deterministic=True)\n",
    "        rewards.append(mean_reward)\n",
    "    \n",
    "    # Save normalization and model\n",
    "    env.save(\"vecnormalize.pkl\")\n",
    "    model.save(\"ppo_model\")\n",
    "\n",
    "    # Load env and model for deterministic simulation\n",
    "    eval_env = DummyVecEnv([make_env(seed)])\n",
    "    eval_env = VecNormalize.load(\"vecnormalize.pkl\", eval_env)\n",
    "    eval_env.training = False\n",
    "    eval_env.norm_reward = False\n",
    "    \n",
    "    model = PPO.load(\"ppo_model\", env=eval_env)\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)  # deterministic=True disables exploration here\n",
    "        obs, reward, done, _ = eval_env.step(action)\n",
    "        eval_env.render()\n",
    "\n",
    "    eval_env.close()\n",
    "    return rewards\n"
   ],
   "id": "7cdcf5da2fd128a3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "total_timesteps = 100_000    # for example\n",
    "eval_interval = 10_000       # evaluate every 10k steps\n",
    "n_eval_episodes = 5          # number of episodes to evaluate\n",
    "\n",
    "def main():\n",
    "    params_B = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"n_steps\": 1024\n",
    "    }\n",
    "    seed = 42\n",
    "    rewards = train_and_evaluate_no_exploration(params_B, seed)\n",
    "\n",
    "    # Optionally: plot rewards here if matplotlib is available\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Evaluation iteration\")\n",
    "    plt.ylabel(\"Mean reward\")\n",
    "    plt.title(\"Training progress\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c82582bcb3a821d7",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
