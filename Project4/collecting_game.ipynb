{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### GRA WŁASNA - COLLECTGAME  \n",
    "## Autorzy : Gabriela Bułat, Gabriela Chmielecka\n",
    "\n",
    "### **1.Wstęp**\n",
    "CollectGame jest to gra, w  której agent ma za zadanie zebrać jak największą ilość owoców omijając przy tym pojawiające się na planszy cukierki.\n",
    "\n",
    "**Przestrzeń rozgrywki**\n",
    "\n",
    "- Plansza 20x20\n",
    "- Pozycja startowa gracza [10,10]\n",
    "- Koniec gry następuje po wykonaniu 125 kroków\n",
    "\n",
    "**Akcja**\n",
    "\n",
    "Gracz, w każdym ruchu  ma do wyboru jedną z pięciu akcji:\n",
    "- 0 - ruch w górę\n",
    "- 1 - ruch w dółł\n",
    "- 2 - ruch w lewo\n",
    "- 3 - ruch w prawo\n",
    "- 4 - pozostanie w miejscu\n",
    "\n",
    "**Nagrody**\n",
    "\n",
    "Przy każdym ruchu przemieszczającym gracza (akcja 0-3) gracz otrzymuje -0.05pkt.\n",
    "Za zdobycie owoca gracz otrzymuje +10pkt.\n",
    "Za zebranie cukierka gracz otrzymuje -10pkt.\n",
    "\n",
    "**Przebieg gry**\n",
    "\n",
    "Gracz poruszając się po planszy stara się zdobyć jak największą liczbę pojawiających się owoców. Owoce jak i cukierki, zostają na planszy do momentu ich \n",
    "zebrania lub do upłynięcia maksymalnego czasu (20 kolejnych kroków). Gracz stara się zminimalizować koszt ruchów, jednocześnie omijając pojawiające się na przestrzeni gry cukierki.\n",
    "\n",
    "**Koniec gry**\n",
    "\n",
    "Gra zostaje zakończona po upłynięciu czasu -wykonanie maksymalnej ilości akcji : 135.\n"
   ],
   "metadata": {},
   "id": "e689178def4ec784"
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import pygame\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ],
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:26:49.473091Z",
     "start_time": "2025-05-25T12:26:49.177426Z"
    }
   },
   "id": "23bd3a0df00e073a",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poniej znajduje się kod implementujący logikę gry. \n",
    "Oto cechy gry poza zwykłą logiką tej gry.\n",
    "\n",
    "#### Obserwacja:\n",
    "\n",
    "Agent otrzymuje obserwację w formie słownika, zawierającą:\n",
    "\n",
    "* aktualną pozycję agenta (`position`),\n",
    "* numer bieżącego kroku (`step`),\n",
    "* wektor przesunięcia do najbliższego obiektu (`target_delta`),\n",
    "* typ najbliższego celu (`target_type`: 1 = owoc, -1 = cukierek, 0 = brak).\n",
    "\n",
    "#### Tryb graficzny:\n",
    "\n",
    "W trybie `\"human\"` środowisko korzysta z biblioteki **Pygame** do wyświetlania planszy i agentów graficznie. Graficzna reprezentacja obejmuje:\n",
    "\n",
    "* siatkę planszy,\n",
    "* agenta (sprite `girl.png`),\n",
    "* owoce (`apple.png`) i cukierki (`lollipop.png`),\n",
    "* pasek informacyjny z aktualnym wynikiem.\n",
    "\n",
    "\n"
   ],
   "metadata": {},
   "id": "6941846d982a3971"
  },
  {
   "cell_type": "code",
   "source": [
    "class CollectEnv(gym.Env):\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\", \"ansi\"]}\n",
    "\n",
    "    ACTIONS = [\"up\", \"down\", \"left\", \"right\", \"none\"]\n",
    "    ACTION_MAP = {\n",
    "        0: (-1, 0),   # up\n",
    "        1: (1, 0),    # down\n",
    "        2: (0, -1),   # left\n",
    "        3: (0, 1),    # right\n",
    "        4: (0, 0),    # none\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.grid_size = (20, 20)\n",
    "        self.cell_size = 20\n",
    "        self.window_size = (self.grid_size[1] * self.cell_size, self.grid_size[0] * self.cell_size + 40)\n",
    "        self.max_steps = 125\n",
    "        self.start_pos = (10, 10)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"position\": spaces.Box(low=0, high=19, shape=(2,), dtype=np.int32),\n",
    "            \"step\": spaces.Discrete(self.max_steps + 1),\n",
    "        })\n",
    "        self.assets = {}\n",
    "        if render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(self.window_size)\n",
    "            self.clock = pygame.time.Clock()\n",
    "            self.load_assets()\n",
    "\n",
    "        self.spawn_schedule: List[Tuple[int, str, Tuple[int, int]]] = [\n",
    "            (0, \"fruit\", (2, 17)), (4, \"candy\", (3, 14)), (7, \"fruit\", (6, 6)),\n",
    "            (17, \"fruit\", (10, 10)), (23, \"candy\", (15, 2)), (24, \"fruit\", (1, 1)),\n",
    "            (27, \"fruit\", (11, 8)), (33, \"fruit\", (19, 19)), (36, \"candy\", (6, 0)),\n",
    "            (37, \"fruit\", (2, 2)), (40, \"candy\", (3, 4)), (33, \"fruit\", (10, 6)),\n",
    "            (45, \"fruit\", (14, 10)), (54, \"fruit\", (15, 2)), (59, \"fruit\", (19, 11)),\n",
    "            (60, \"candy\", (8, 8)), (63, \"candy\", (7, 13)), (66, \"fruit\", (1, 6)),\n",
    "            (75, \"fruit\", (4, 13)), (79, \"candy\", (9, 19)), (82, \"fruit\", (8, 15)),\n",
    "            (83, \"fruit\", (10, 10)), (84, \"candy\", (17, 12)), (89, \"fruit\", (14, 11)),\n",
    "            (94, \"fruit\", (8, 8)), (98, \"fruit\", (5, 13)), (99, \"candy\", (7, 14)),\n",
    "            (105, \"fruit\", (10, 16)), (111, \"candy\", (8, 19)), (115, \"fruit\", (7, 19)),\n",
    "        ]\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agent_pos = list(self.start_pos)\n",
    "        self.current_step = 0\n",
    "        self.score = 0\n",
    "        self.objects: Dict[Tuple[int, int], Tuple[str, int]] = {}\n",
    "        self.game_over = False\n",
    "        self.game_over_time = None\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action: int):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "\n",
    "        # Spawn new objects\n",
    "        for spawn_time, obj_type, pos in self.spawn_schedule:\n",
    "            if spawn_time == self.current_step:\n",
    "                self.objects[pos] = (obj_type, self.current_step)\n",
    "\n",
    "                # Remove expired objects (after 30 steps)\n",
    "        expired_positions = [\n",
    "            pos for pos, (_, spawn_step) in self.objects.items()\n",
    "            if self.current_step - spawn_step >= 30\n",
    "        ]\n",
    "        for pos in expired_positions:\n",
    "            del self.objects[pos]\n",
    "\n",
    "        # Move agent\n",
    "        delta = self.ACTION_MAP[action]\n",
    "        new_row = min(max(self.agent_pos[0] + delta[0], 0), self.grid_size[0] - 1)\n",
    "        new_col = min(max(self.agent_pos[1] + delta[1], 0), self.grid_size[1] - 1)\n",
    "        self.agent_pos = [new_row, new_col]\n",
    "\n",
    "        # Check for item pickup\n",
    "        reward = 0\n",
    "        pos_tuple = tuple(self.agent_pos)\n",
    "        if pos_tuple in self.objects:\n",
    "            obj_type, _ = self.objects[pos_tuple]\n",
    "            if obj_type == \"fruit\":\n",
    "                reward += 10\n",
    "            elif obj_type == \"candy\":\n",
    "                reward -= 10\n",
    "            del self.objects[pos_tuple]\n",
    "\n",
    "        # Action penalty\n",
    "        if action in [0, 1, 2, 3]:  # up/down/left/right\n",
    "            reward -= 0.05\n",
    "\n",
    "        self.score += reward\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        self.game_over = terminated\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        pos = np.array(self.agent_pos, dtype=np.int32)\n",
    "\n",
    "        # Znajdź najbliższy cel\n",
    "        min_dist = float(\"inf\")\n",
    "        target_delta = (0, 0)\n",
    "        target_type = 0  # 0 = brak, 1 = fruit, -1 = candy\n",
    "\n",
    "        for (obj_x, obj_y), (obj_type, _) in self.objects.items():\n",
    "            dx = obj_x - self.agent_pos[0]\n",
    "            dy = obj_y - self.agent_pos[1]\n",
    "            dist = abs(dx) + abs(dy)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                target_delta = (dx, dy)\n",
    "                target_type = 1 if obj_type == \"fruit\" else -1\n",
    "\n",
    "        return {\n",
    "            \"position\": pos,\n",
    "            \"step\": self.current_step,\n",
    "            \"target_delta\": np.array(target_delta, dtype=np.int32),\n",
    "            \"target_type\": target_type,\n",
    "        }\n",
    "\n",
    "    def load_assets(self):\n",
    "        self.assets[\"agent\"] = pygame.image.load(os.path.join(\"pictures\", \"girl.png\"))\n",
    "        self.assets[\"fruit\"] = pygame.image.load(os.path.join(\"pictures\", \"apple.png\"))\n",
    "        self.assets[\"candy\"] = pygame.image.load(os.path.join(\"pictures\", \"lollipop.png\"))\n",
    "\n",
    "        # Scale them to fit cell size\n",
    "        for key in self.assets:\n",
    "            self.assets[key] = pygame.transform.scale(self.assets[key], (self.cell_size, self.cell_size))\n",
    "\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return super().render()\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(self.window_size)\n",
    "            pygame.display.set_caption(\"CollectEnv GUI\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "            self.font = pygame.font.SysFont(\"Arial\", 20)\n",
    "\n",
    "        self.window.fill((255, 255, 255))  # tło\n",
    "\n",
    "        # punkty\n",
    "        score_text = self.font.render(f\"Punkty: {self.score:.1f}\", True, (0, 0, 0))\n",
    "        self.window.blit(score_text, (10, 10))\n",
    "\n",
    "        offset_y = 40\n",
    "\n",
    "        # Siatka\n",
    "        for x in range(0, self.window_size[0], self.cell_size):\n",
    "            pygame.draw.line(self.window, (200, 200, 200), (x, offset_y), (x, self.window_size[1]))\n",
    "        for y in range(offset_y, self.window_size[1], self.cell_size):\n",
    "            pygame.draw.line(self.window, (200, 200, 200), (0, y), (self.window_size[0], y))\n",
    "\n",
    "        # Obiekty \n",
    "        for (x, y), item in self.objects.items():\n",
    "            obj_type, _ = item\n",
    "            sprite = self.assets[\"fruit\"] if obj_type == \"fruit\" else self.assets[\"candy\"]\n",
    "            self.window.blit(sprite, (y * self.cell_size, x * self.cell_size + offset_y))\n",
    "\n",
    "        # Agent \n",
    "        ax, ay = self.agent_pos\n",
    "        self.window.blit(self.assets[\"agent\"], (ay * self.cell_size, ax * self.cell_size + offset_y))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(10)\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.quit()\n",
    "            self.window = None\n"
   ],
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:26:49.834029Z",
     "start_time": "2025-05-25T12:26:49.500718Z"
    }
   },
   "id": "5b1fa6cd28ab3c2f",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Poniższy kod uruchamia grę w trybie graficznym - zachęcamy do podjęcia próby złapania jak najwięcej owocków !**\n",
    "\n",
    "W celu przyjemniejszego grania klawisze ruchów zostały zmienione z cyfr na strzałki:\n",
    "\n",
    "- strzałka górna - ruch w góre (zmiana z 0)\n",
    "- strzałka dolna - ruch w dół (zmiana z 1)\n",
    "- strzałka lewa - ruch w lewo (zmiana z 2)\n",
    "- strzałka prawo - ruch w prawo (zmiana z 3)\n",
    "- cyfra 4 - brak ruchu (niezmienione z wersji oryginalnej, gdyż rzadko użwane)\n",
    "\n",
    "W części graficznej widzmy naszą przestrzeń gry, podzielona na siatkę w celu łatwego określenia odległości od celi. Poruszamy się postacią dziewczynki zbierając pojawiające się zdrowe jabłka, omijając po drodze niezdrowe słodycze."
   ],
   "metadata": {},
   "id": "b02a66f479bf7e73"
  },
  {
   "cell_type": "code",
   "source": [
    "env = CollectEnv(render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "            break\n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_ESCAPE:\n",
    "                running = False\n",
    "                break\n",
    "\n",
    "            action = 4  # brak ruchu\n",
    "            if event.key == pygame.K_UP:\n",
    "                action = 0\n",
    "            elif event.key == pygame.K_DOWN:\n",
    "                action = 1\n",
    "            elif event.key == pygame.K_LEFT:\n",
    "                action = 2\n",
    "            elif event.key == pygame.K_RIGHT:\n",
    "                action = 3\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            env.render()\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Koniec gry. Zdobyte punkty: {env.score:.2f}. Resetuję...\")\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "env.close()\n",
    "pygame.quit()\n",
    "sys.exit()"
   ],
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:33:56.727368Z",
     "start_time": "2025-05-25T12:33:11.456797Z"
    }
   },
   "id": "35b4e25d80668946",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nasze GUI\n",
   "id": "fa3288e431aa3d7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:46:59.996588Z",
     "start_time": "2025-06-10T14:46:59.838998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "pc1 = cv2.imread(r'gui\\img.png')\n",
    "pc2 = cv2.imread(r'gui\\img_1.png')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(pc1)\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pc2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "abb377689be0829a",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Przekształcanie obserwacji na stan dyskretny\n",
    "\n",
    "Poniższa funkcja `extract_state` przekształca obserwację otrzymaną ze środowiska `CollectEnv` do postaci skwantyzowanego stanu, który może być użyty przez algorytmy tablicowe takie jak Q-learning lub SARSA. Wektory przesunięcia do celu (`dx`, `dy`) są ograniczane do zakresu od -10 do 10, aby zmniejszyć przestrzeń stanów i poprawić efektywność uczenia. Wynikiem funkcji jest krotka zawierająca pozycję agenta, przycięte różnice pozycji względem celu oraz typ najbliższego obiektu.\n"
   ],
   "metadata": {},
   "id": "926bfc1e"
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_state(obs):\n",
    "    \"\"\"\n",
    "    Przekształca obserwację środowiska do reprezentacji stanu dla algorytmu Q-learning/SARSA.\n",
    "    Przycinamy różnicę pozycji celu do [-10, 10], aby ograniczyć przestrzeń stanów.\n",
    "\n",
    "    Args:\n",
    "        obs (dict): obserwacja ze środowiska zawierająca pozycję agenta, różnicę do celu i typ celu.\n",
    "\n",
    "    Returns:\n",
    "        tuple: skwantyzowany stan jako krotka (x, y, dx, dy, typ celu)\n",
    "    \"\"\"\n",
    "    dx = np.clip(obs[\"target_delta\"][0], -10, 10)\n",
    "    dy = np.clip(obs[\"target_delta\"][1], -10, 10)\n",
    "    return (\n",
    "        obs[\"position\"][0],\n",
    "        obs[\"position\"][1],\n",
    "        dx,\n",
    "        dy,\n",
    "        obs[\"target_type\"]\n",
    "    )"
   ],
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:44:13.671607Z",
     "start_time": "2025-05-25T12:44:13.658808Z"
    }
   },
   "id": "e0ebf579fb11141d",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Strategia wyboru akcji: epsilon-greedy\n",
    "\n",
    "Poniższa funkcja `epsilon_greedy_policy` implementuje strategię eksploracyjno-eksploatacyjną epsilon-greedy, stosowaną w uczeniu ze wzmocnieniem. Z określonym prawdopodobieństwem `epsilon` wybierana jest losowa akcja (eksploracja), natomiast w pozostałych przypadkach wybierana jest akcja maksymalizująca wartość Q w danym stanie (eksploatacja). Strategia ta pozwala agentowi eksplorować nowe stany, jednocześnie stopniowo skupiając się na działaniach dających największe oczekiwane nagrody.\n"
   ],
   "metadata": {},
   "id": "084d8666"
  },
  {
   "cell_type": "code",
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon, n_actions):\n",
    "    \"\"\"\n",
    "    Strategia eksploracyjna epsilon-greedy: z prawdopodobieństwem epsilon wybierz losową akcję,\n",
    "    w przeciwnym razie wybierz akcję maksymalizującą wartość Q.\n",
    "\n",
    "    Args:\n",
    "        Q (dict): tablica Q.\n",
    "        state (tuple): obecny stan.\n",
    "        epsilon (float): współczynnik eksploracji.\n",
    "        n_actions (int): liczba możliwych akcji.\n",
    "\n",
    "    Returns:\n",
    "        int: wybrana akcja.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    return np.argmax(Q[state])"
   ],
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:44:24.317552Z",
     "start_time": "2025-05-25T12:44:24.301851Z"
    }
   },
   "id": "18e94c4cea6cbfaa",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trening agenta za pomocą algorytmu Q-learning\n",
    "\n",
    "Poniższa funkcja `train_q_learning` implementuje algorytm **Q-learning**. Agent uczy się, które akcje prowadzą do najwyższych nagród, na podstawie aktualizacji wartości Q dla odwiedzanych stanów i wykonywanych akcji.\n",
    "\n",
    "W trakcie treningu agent, przez określoną liczbę epizodów, eksploruje środowisko i aktualizuje tablicę Q według reguły:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "Główne parametry funkcji:\n",
    "\n",
    "* `alpha` – tempo uczenia (jak szybko agent aktualizuje wiedzę),\n",
    "* `gamma` – współczynnik dyskontujący przyszłe nagrody,\n",
    "* `epsilon` – prawdopodobieństwo eksploracji (czyli losowego wyboru akcji).\n",
    "\n",
    "Funkcja zwraca:\n",
    "\n",
    "* wytrenowaną tablicę Q,\n",
    "* listę nagród z każdego epizodu,\n",
    "* listę czasów wykonania epizodów (przydatne do analizy wydajności).\n"
   ],
   "metadata": {},
   "id": "1703546d"
  },
  {
   "cell_type": "code",
   "source": [
    "def train_q_learning(env, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Trenuje agenta za pomocą algorytmu Q-learning w podanym środowisku.\n",
    "\n",
    "    Args:\n",
    "        env: środowisko (np. CollectEnv).\n",
    "        episodes (int): liczba epizodów treningowych.\n",
    "        alpha (float): współczynnik uczenia.\n",
    "        gamma (float): współczynnik dyskontujący przyszłe nagrody.\n",
    "        epsilon (float): prawdopodobieństwo eksploracji.\n",
    "\n",
    "    Returns:\n",
    "        Q: wytrenowana tablica Q.\n",
    "        episode_rewards: lista sum nagród z każdego epizodu.\n",
    "        episode_times: lista czasów trwania epizodów.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    episode_rewards = []\n",
    "    episode_times = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = extract_state(obs)\n",
    "        total_reward = 0\n",
    "        start_time = time.time()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon, env.action_space.n)\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = extract_state(next_obs)\n",
    "\n",
    "            # Q-learning update rule\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "            Q[state][action] += alpha * (td_target - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_times.append(time.time() - start_time)\n",
    "\n",
    "        if (ep + 1) % 100 == 0:\n",
    "            print(f\"Q-Learning - Episode {ep + 1}, reward: {total_reward:.2f}\")\n",
    "\n",
    "    return Q, episode_rewards, episode_times\n"
   ],
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:46:25.479641Z",
     "start_time": "2025-05-25T12:46:25.463680Z"
    }
   },
   "id": "d9b35cbd55557f",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trening agenta za pomocą algorytmu SARSA\n",
    "\n",
    "Poniższa funkcja `train_sarsa` implementuje algorytm **SARSA** (*State-Action-Reward-State-Action*), będący metodą on-policy w uczeniu ze wzmocnieniem. W przeciwieństwie do Q-learningu, który wykorzystuje optymalną wartość w przyszłym stanie, SARSA aktualizuje tablicę Q na podstawie akcji, którą agent **faktycznie** podejmie w kolejnym kroku.\n",
    "\n",
    "Formuła aktualizacji wartości Q wygląda następująco:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\cdot Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "Podobnie jak w Q-learningu:\n",
    "\n",
    "* `alpha` kontroluje tempo uczenia,\n",
    "* `gamma` określa wagę przyszłych nagród,\n",
    "* `epsilon` odpowiada za eksplorację środowiska.\n",
    "\n",
    "Funkcja uruchamia trening przez zadaną liczbę epizodów i zwraca:\n",
    "\n",
    "* wyuczoną tablicę Q,\n",
    "* listę nagród z każdego epizodu (do analizy postępu uczenia),\n",
    "* czas trwania każdego epizodu (do pomiarów wydajnościowych).\n"
   ],
   "metadata": {},
   "id": "cd103416"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def train_sarsa(env, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Trenuje agenta za pomocą algorytmu SARSA.\n",
    "\n",
    "    Args:\n",
    "        env: środowisko.\n",
    "        episodes (int): liczba epizodów.\n",
    "        alpha (float): współczynnik uczenia.\n",
    "        gamma (float): współczynnik dyskontujący przyszłe nagrody.\n",
    "        epsilon (float): prawdopodobieństwo eksploracji.\n",
    "\n",
    "    Returns:\n",
    "        Q: wytrenowana tablica Q.\n",
    "        episode_rewards: lista sum nagród.\n",
    "        episode_times: lista czasów trwania epizodów.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    episode_rewards = []\n",
    "    episode_times = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = extract_state(obs)\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, env.action_space.n)\n",
    "\n",
    "        total_reward = 0\n",
    "        start_time = time.time()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = extract_state(next_obs)\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, env.action_space.n)\n",
    "\n",
    "            # SARSA update rule\n",
    "            td_target = reward + gamma * Q[next_state][next_action]\n",
    "            Q[state][action] += alpha * (td_target - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_times.append(time.time() - start_time)\n",
    "\n",
    "        if (ep + 1) % 100 == 0:\n",
    "            print(f\"SARSA - Episode {ep + 1}, reward: {total_reward:.2f}\")\n",
    "\n",
    "    return Q, episode_rewards, episode_times"
   ],
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:44:27.187252Z",
     "start_time": "2025-05-25T12:44:27.164736Z"
    }
   },
   "id": "6ff871ef6d0e4c7e",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wizualizacja wyników treningu agenta\n",
    "\n",
    "Poniższa funkcja `plot_training` odpowiada za graficzną prezentację wyników treningu agenta. Generuje dwa wykresy dla wybranej metody uczenia (np. Q-learning lub SARSA):\n",
    "\n",
    "1. **Nagroda w funkcji epizodu** – pokazuje, jak zmieniała się suma nagród uzyskanych przez agenta w kolejnych epizodach. Pozwala ocenić, czy agent uczy się skutecznie zbierać dobre przedmioty i unikać złych.\n",
    "\n",
    "2. **Czas trwania epizodu** – pozwala zaobserwować, jak długo trwały kolejne epizody treningowe. Może to być pomocne do analizy wydajności lub złożoności decyzji podejmowanych przez agenta.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {},
   "id": "8f0b9503"
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_training(rewards, times, method_name):\n",
    "    \"\"\"\n",
    "    Generuje i wyświetla wykresy postępu treningu: nagroda i czas trwania epizodu.\n",
    "\n",
    "    Args:\n",
    "        rewards (list): nagrody z epizodów.\n",
    "        times (list): czasy epizodów.\n",
    "        method_name (str): nazwa metody (np. 'Q-Learning').\n",
    "    \"\"\"\n",
    "    episodes = list(range(1, len(rewards) + 1))\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Reward vs Episode\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episodes, rewards, label=\"Reward\", color=\"blue\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(f\"{method_name}: Reward vs Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Time vs Episode\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(episodes, times, label=\"Time\", color=\"green\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(f\"{method_name}: Time vs Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ],
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:44:28.129798Z",
     "start_time": "2025-05-25T12:44:28.111079Z"
    }
   },
   "id": "6fb7a62f7a69623d",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Rezultaty uruchomienia powyższych agentów"
   ],
   "metadata": {},
   "id": "e55610b6"
  },
  {
   "cell_type": "code",
   "source": [
    "env = CollectEnv(render_mode=None)\n"
   ],
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:44:29.215952Z",
     "start_time": "2025-05-25T12:44:29.206Z"
    }
   },
   "id": "61964517d105ce34",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-learning"
   ],
   "metadata": {},
   "id": "09472b6f"
  },
  {
   "cell_type": "code",
   "source": [
    "# Q-learning\n",
    "q_table, q_rewards, q_times = train_q_learning(env, episodes=5000)\n",
    "plot_training(q_rewards, q_times, method_name=\"Q-Learning\")"
   ],
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:47:09.110780Z",
     "start_time": "2025-05-25T12:46:33.568487Z"
    }
   },
   "id": "17a97264e2826e87",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SARSA"
   ],
   "metadata": {},
   "id": "2e3d6ba9"
  },
  {
   "cell_type": "code",
   "source": [
    "#SARSA \n",
    "sarsa_table, sarsa_rewards, sarsa_times = train_sarsa(env, episodes=5000)\n",
    "plot_training(sarsa_rewards, sarsa_times, method_name=\"SARSA\")"
   ],
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:45:20.722630Z",
     "start_time": "2025-05-25T12:44:44.557593Z"
    }
   },
   "id": "ebeac20a849771d4",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Wnioski\n",
    "\n",
    "1. **Poprawne działanie środowiska**\n",
    "   Samodzielnie zaprojektowane środowisko `CollectEnv` działa zgodnie z założeniami – agent porusza się po siatce, zbiera dobre obiekty (`fruit`) i unika złych (`candy`). Mechanizm pojawiania się i wygasania obiektów oraz system punktacji odzwierciedlają realistyczne wyzwanie decyzyjne.\n",
    "\n",
    "2. **Efektywność algorytmów uczenia ze wzmocnieniem**\n",
    "   Zarówno algorytm **Q-learning**, jak i **SARSA** umożliwiły agentowi nauczenie się sensownej polityki działania. Z każdym kolejnym epizodem agent uzyskiwał coraz wyższe nagrody, co wskazuje na skuteczność procesu uczenia.\n",
    "\n",
    "3. **Porównanie Q-learning vs SARSA**\n",
    "\n",
    "   * Q-learning jako metoda off-policy często prowadził do szybszego wzrostu sumy nagród w początkowych epizodach.\n",
    "   * SARSA jako metoda on-policy generowała bardziej stabilne, ale nieco wolniejsze tempo uczenia – co może być korzystne w środowiskach o większym ryzyku (np. z dużą karą za błędne decyzje).\n",
    "   * Różnice między algorytmami były zauważalne, ale nie drastyczne – obie metody osiągały dobre wyniki w dłuższej perspektywie.\n",
    "\n",
    "4. **Eksploracja a eksploatacja**\n",
    "   Zastosowanie strategii epsilon-greedy pozwoliło zachować balans pomiędzy eksploracją nowych stanów a eksploatacją już poznanych, co było kluczowe dla skutecznego treningu.\n",
    "\n",
    "5. **Wizualizacja i analiza**\n",
    "   Wykresy postępu treningu (nagroda/epizod, czas/epizod) dostarczyły cennych informacji diagnostycznych – m.in. pozwoliły zidentyfikować momenty stagnacji lub gwałtownego postępu w nauce agenta.\n",
    "\n"
   ],
   "metadata": {},
   "id": "120e026470fd1cf2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "kernel_info": {
   "name": "python"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
